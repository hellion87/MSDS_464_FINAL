
@article{adamski_distributed_2018,
	title = {Distributed Deep Reinforcement Learning: Learn how to play Atari games in 21 minutes},
	url = {http://arxiv.org/abs/1801.02852},
	shorttitle = {Distributed Deep Reinforcement Learning},
	abstract = {We present a study in Distributed Deep Reinforcement Learning ({DDRL}) focused on scalability of a state-of-the-art Deep Reinforcement Learning algorithm known as Batch Asynchronous Advantage {ActorCritic} ({BA}3C). We show that using the Adam optimization algorithm with a batch size of up to 2048 is a viable choice for carrying out large scale machine learning computations. This, combined with careful reexamination of the optimizer’s hyperparameters, using synchronous training on the node level (while keeping the local, single node part of the algorithm asynchronous) and minimizing the model’s memory footprint, allowed us to achieve linear scaling for up to 64 {CPU} nodes. This corresponds to a training time of 21 minutes on 768 {CPU} cores, as opposed to the 10 hours required when using a single node with 24 cores achieved by a baseline single-node implementation.},
	journaltitle = {{arXiv}:1801.02852 [cs]},
	author = {Adamski, Igor and Adamski, Robert and Grel, Tomasz and Jędrych, Adam and Kaczmarek, Kamil and Michalewski, Henryk},
	urldate = {2021-11-08},
	date = {2018-04-09},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.02852},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Adamski et al. - 2018 - Distributed Deep Reinforcement Learning Learn how.pdf:/Users/philmoremorrison/Zotero/storage/L9UA7FZG/Adamski et al. - 2018 - Distributed Deep Reinforcement Learning Learn how.pdf:application/pdf},
}

@article{david_deepchess_2016,
	title = {{DeepChess}: End-to-End Deep Neural Network for Automatic Learning in Chess},
	volume = {9887},
	url = {http://arxiv.org/abs/1711.09667},
	doi = {10.1007/978-3-319-44781-0_11},
	shorttitle = {{DeepChess}},
	abstract = {We present an end-to-end learning method for chess, relying on deep neural networks. Without any a priori knowledge, in particular without any knowledge regarding the rules of chess, a deep neural network is trained using a combination of unsupervised pretraining and supervised training. The unsupervised training extracts high level features from a given position, and the supervised training learns to compare two chess positions and select the more favorable one. The training relies entirely on datasets of several million chess games, and no further domain speciﬁc knowledge is incorporated.},
	pages = {88--96},
	journaltitle = {{arXiv}:1711.09667 [cs, stat]},
	author = {David, Eli and Netanyahu, Nathan S. and Wolf, Lior},
	urldate = {2021-11-08},
	date = {2016},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1711.09667},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {David et al. - 2016 - DeepChess End-to-End Deep Neural Network for Auto.pdf:/Users/philmoremorrison/Zotero/storage/8EJUULKA/David et al. - 2016 - DeepChess End-to-End Deep Neural Network for Auto.pdf:application/pdf},
}

@article{francois-lavet_how_2016,
	title = {How to Discount Deep Reinforcement Learning: Towards New Dynamic Strategies},
	url = {http://arxiv.org/abs/1512.02011},
	shorttitle = {How to Discount Deep Reinforcement Learning},
	abstract = {Using deep neural nets as function approximator for reinforcement learning tasks have recently been shown to be very powerful for solving problems approaching real-world complexity such as [1]. Using these results as a benchmark, we discuss the role that the discount factor may play in the quality of the learning process of a deep Q-network ({DQN}). When the discount factor progressively increases up to its ﬁnal value, we empirically show that it is possible to signiﬁcantly reduce the number of learning steps. When used in conjunction with a varying learning rate, we empirically show that it outperforms original {DQN} on several experiments. We relate this phenomenon with the instabilities of neural networks when they are used in an approximate Dynamic Programming setting. We also describe the possibility to fall within a local optimum during the learning process, thus connecting our discussion with the exploration/exploitation dilemma.},
	journaltitle = {{arXiv}:1512.02011 [cs]},
	author = {François-Lavet, Vincent and Fonteneau, Raphael and Ernst, Damien},
	urldate = {2021-11-08},
	date = {2016-01-20},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1512.02011},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {François-Lavet et al. - 2016 - How to Discount Deep Reinforcement Learning Towar.pdf:/Users/philmoremorrison/Zotero/storage/PXPVT86S/François-Lavet et al. - 2016 - How to Discount Deep Reinforcement Learning Towar.pdf:application/pdf},
}

@article{lai_giraffe_2015,
	title = {Giraffe: Using Deep Reinforcement Learning to Play Chess},
	url = {http://arxiv.org/abs/1509.01549},
	shorttitle = {Giraffe},
	abstract = {This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.},
	journaltitle = {{arXiv}:1509.01549 [cs]},
	author = {Lai, Matthew},
	urldate = {2021-11-08},
	date = {2015-09-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1509.01549},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Artificial Intelligence},
	file = {Lai - 2015 - Giraffe Using Deep Reinforcement Learning to Play.pdf:/Users/philmoremorrison/Zotero/storage/Q266T6QL/Lai - 2015 - Giraffe Using Deep Reinforcement Learning to Play.pdf:application/pdf},
}


@article{baxter_learning_2000,
author = {Baxter, Jonathan and Tridgell, Andrew and Weaver, Lex},
year = {2000},
month = {09},
pages = {243-263},
title = {Learning to Play Chess Using Temporal Differences},
volume = {40},
journal = {Machine Learning},
doi = {10.1023/A:1007634325138}
}

@article{silver_mastering_2017,
	title = {Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm},
	url = {http://arxiv.org/abs/1712.01815},
	abstract = {The game of chess is the most widely-studied domain in the history of artiﬁcial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-speciﬁc adaptations, and handcrafted evaluation functions that have been reﬁned by human experts over several decades. In contrast, the {AlphaGo} Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single {AlphaZero} algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, {AlphaZero} achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
	journaltitle = {{arXiv}:1712.01815 [cs]},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	urldate = {2021-11-08},
	date = {2017-12-05},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1712.01815},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:/Users/philmoremorrison/Zotero/storage/Y4Q4KZIW/Silver et al. - 2017 - Mastering Chess and Shogi by Self-Play with a Gene.pdf:application/pdf},
}

@article{schaff_jointly_2018,
	title = {Jointly Learning to Construct and Control Agents using Deep Reinforcement Learning},
	url = {http://arxiv.org/abs/1801.01432},
	abstract = {The physical design of a robot and the policy that controls its motion are inherently coupled, and should be determined according to the task and environment. In an increasing number of applications, data-driven and learningbased approaches, such as deep reinforcement learning, have proven effective at designing control policies. For most tasks, the only way to evaluate a physical design with respect to such control policies is empirical—i.e., by picking a design and training a control policy for it. Since training these policies is timeconsuming, it is computationally infeasible to train separate policies for all possible designs as a means to identify the best one. In this work, we address this limitation by introducing a method that performs simultaneous joint optimization of the physical design and control network. Our approach maintains a distribution over designs and uses reinforcement learning to optimize a control policy to maximize expected reward over the design distribution. We give the controller access to design parameters to allow it to tailor its policy to each design in the distribution. Throughout training, we shift the distribution towards higher-performing designs, eventually converging to a design and control policy that are jointly optimal. We evaluate our approach in the context of legged locomotion, and demonstrate that it discovers novel designs and walking gaits, outperforming baselines in both performance and efﬁciency.},
	journaltitle = {{arXiv}:1801.01432 [cs]},
	author = {Schaff, Charles and Yunis, David and Chakrabarti, Ayan and Walter, Matthew R.},
	urldate = {2021-11-08},
	date = {2018-09-14},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1801.01432},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Schaff et al. - 2018 - Jointly Learning to Construct and Control Agents u.pdf:/Users/philmoremorrison/Zotero/storage/MARWACQT/Schaff et al. - 2018 - Jointly Learning to Construct and Control Agents u.pdf:application/pdf},
}

@article{silver_general_2018,
	title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
	volume = {362},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.aar6404},
	doi = {10.1126/science.aar6404},
	abstract = {The game of chess is the longest-studied domain in the history of artiﬁcial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-speciﬁc adaptations, and handcrafted evaluation functions that have been reﬁned by human experts over several decades. By contrast, the {AlphaGo} Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from selfplay. In this paper, we generalize this approach into a single {AlphaZero} algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, {AlphaZero} convincingly defeated a world champion program in the games of chess and shogi (Japanese chess) as well as Go.},
	pages = {1140--1144},
	number = {6419},
	journaltitle = {Science},
	shortjournal = {Science},
	author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
	urldate = {2021-11-08},
	date = {2018-12-07},
	langid = {english},
	file = {Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:/Users/philmoremorrison/Zotero/storage/JBXFNC4V/Silver et al. - 2018 - A general reinforcement learning algorithm that ma.pdf:application/pdf},
}

@article{campbell_deep_2002,
	title = {Deep Blue},
	volume = {134},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370201001291},
	doi = {10.1016/S0004-3702(01)00129-1},
	pages = {57--83},
	number = {1},
	journaltitle = {Artificial Intelligence},
	shortjournal = {Artificial Intelligence},
	author = {Campbell, Murray and Hoane, A.Joseph and Hsu, Feng-hsiung},
	urldate = {2021-11-08},
	date = {2002-01},
	langid = {english},
	file = {Deep Blue.pdf:/Users/philmoremorrison/Zotero/storage/4R7AYQXA/Deep Blue.pdf:application/pdf},
}

@article{lohokare_deep_nodate,
	title = {Deep Learning Bot for League of Legends},
	abstract = {In this paper, we take the ﬁrst step towards building comprehensive bots capable of playing a simpliﬁed version of League of Legends, a popular 5v5 online multiplayer game. We implement two different agents, one using Reinforcement Learning and the other via Supervised Long Short Term Memory Network. League of Legends provides a partially observable game environment with an action space much larger than games like Chess or Go. Source code and demonstrations can be found at https://github.com/csci-599-appliedml-for-games/league-of-legends-bot.},
	pages = {3},
	author = {Lohokare, Aishwarya},
	langid = {english},
	file = {Lohokare - Deep Learning Bot for League of Legends.pdf:/Users/philmoremorrison/Zotero/storage/Z43I2EIJ/Lohokare - Deep Learning Bot for League of Legends.pdf:application/pdf},
}

@misc{swiechowski2021,
      title={Monte Carlo Tree Search: A Review of Recent Modifications and Applications}, 
      author={Maciej Świechowski and Konrad Godlewski and Bartosz Sawicki and Jacek Mańdziuk},
      year={2021},
      eprint={2103.04931},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}

@ARTICLE{samuel1959,
  author={Samuel, A. L.},
  journal={IBM Journal of Research and Development}, 
  title={Some Studies in Machine Learning Using the Game of Checkers}, 
  year={1959},
  volume={3},
  number={3},
  pages={210-229},
  doi={10.1147/rd.33.0210}
  }
  
  @article{sutton1988learning,
  title={Learning to Predict by The Methods of Temporal Differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  number={1},
  pages={9--44},
  year={1988},
  publisher={Springer}
}


@article{tesauro_temporal_nodate,
	title = {Temporal Difference Learning and {TD}-Gammon},
	pages = {16},
	author = {Tesauro, Gerald},
	langid = {english},
	file = {Tesauro - Temporal Difference Learning and TD-Gammon.pdf:/Users/philmoremorrison/Zotero/storage/LJG5M9JK/Tesauro - Temporal Difference Learning and TD-Gammon.pdf:application/pdf},
}

@online{science_understanding_2021,
	title = {Understanding the Temporal Difference Learning and its Predication},
	url = {https://medium.com/@ODSC/understanding-the-temporal-difference-learning-and-its-predication-efc91016144a},
	abstract = {The temporal difference learning algorithm was introduced by Richard S. Sutton in 1988. The reason the temporal difference learning method…},
	titleaddon = {Medium},
	author = {Science, {ODSC}-Open Data},
	urldate = {2021-11-08},
	date = {2021-01-11},
	langid = {english},
	file = {Snapshot:/Users/philmoremorrison/Zotero/storage/VUTP45XB/understanding-the-temporal-difference-learning-and-its-predication-efc91016144a.html:text/html},
}

@online{agent_temporal_2020,
	title = {Temporal Difference Learning —},
	url = {https://medium.com/swlh/temporal-difference-learning-62cac48e019f},
	abstract = {In this article, let us look at Temporal Difference Learning, a learning method that unlike Monte Carlo methods, does not need an episode…},
	titleaddon = {The Startup},
	author = {Agent, The Perceptive},
	urldate = {2021-11-08},
	date = {2020-05-03},
	langid = {english},
	file = {Snapshot:/Users/philmoremorrison/Zotero/storage/FNCHP6M7/temporal-difference-learning-62cac48e019f.html:text/html},
}

@article{gunther_examining_2020,
	title = {Examining the Use of Temporal-Difference Incremental Delta-Bar-Delta for Real-World Predictive Knowledge Architectures},
	volume = {7},
	issn = {2296-9144},
	url = {https://www.frontiersin.org/article/10.3389/frobt.2020.00034},
	doi = {10.3389/frobt.2020.00034},
	abstract = {Predictions and predictive knowledge have seen recent success in improving not only robot control but also other applications ranging from industrial process control to rehabilitation. A property that makes these predictive approaches well-suited for robotics is that they can be learned online and incrementally through interaction with the environment. However, a remaining challenge for many prediction-learning approaches is an appropriate choice of prediction-learning parameters, especially parameters that control the magnitude of a learning machine's updates to its predictions (the learning rates or step sizes). Typically, these parameters are chosen based on an extensive parameter search—an approach that neither scales well nor is well-suited for tasks that require changing step sizes due to non-stationarity. To begin to address this challenge, we examine the use of online step-size adaptation using the Modular Prosthetic Limb: a sensor-rich robotic arm intended for use by persons with amputations. Our method of choice, Temporal-Difference Incremental Delta-Bar-Delta ({TIDBD}), learns and adapts step sizes on a feature level; importantly, {TIDBD} allows step-size tuning and representation learning to occur at the same time. As a first contribution, we show that {TIDBD} is a practical alternative for classic Temporal-Difference ({TD}) learning via an extensive parameter search. Both approaches perform comparably in terms of predicting future aspects of a robotic data stream, but {TD} only achieves comparable performance with a carefully hand-tuned learning rate, while {TIDBD} uses a robust meta-parameter and tunes its own learning rates. Secondly, our results show that for this particular application {TIDBD} allows the system to automatically detect patterns characteristic of sensor failures common to a number of robotic applications. As a third contribution, we investigate the sensitivity of classic {TD} and {TIDBD} with respect to the initial step-size values on our robotic data set, reaffirming the robustness of {TIDBD} as shown in previous papers. Together, these results promise to improve the ability of robotic devices to learn from interactions with their environments in a robust way, providing key capabilities for autonomous agents and robots.},
	pages = {34},
	journaltitle = {Frontiers in Robotics and {AI}},
	author = {Günther, Johannes and Ady, Nadia M. and Kearney, Alex and Dawson, Michael R. and Pilarski, Patrick M.},
	urldate = {2021-11-08},
	date = {2020},
	file = {Full Text PDF:/Users/philmoremorrison/Zotero/storage/G25NEJ8D/Günther et al. - 2020 - Examining the Use of Temporal-Difference Increment.pdf:application/pdf},
}

@article{varga_prediction_2019,
	title = {Prediction of Electric Vehicle Range: A Comprehensive Review of Current Issues and Challenges},
	volume = {12},
	issn = {1996-1073},
	url = {https://www.mdpi.com/1996-1073/12/5/946},
	doi = {10.3390/en12050946},
	shorttitle = {Prediction of Electric Vehicle Range},
	abstract = {Electric vehicles ({EV}) are the immediate solution to drastically reducing pollutant emissions from the transport sector. There is a continuing increase in the number of {EVs} in use, but their widespread and massive acceptance by automotive consumers is related to the performance they can deliver. The most important feature here (a hot topic at present in {EV} research) is related to the possibility of providing a more accurate prediction of range. Range prediction is a complex problem because it depends on a lot of inﬂuence factors (internal, external, constant, variables) and the present paper aims to investigate the effect of these factors on the range of {EVs}. The results and aspects of current worldwide research on this theme are presented through the analysis of the main classes of inﬂuence factors: Vehicle design, the driver and the environment. Further, the weight and effect of each potential factor which inﬂuences {EV} range was analyzed by presenting current issues. An exhaustive and comprehensive analysis has made it possible to identify future research and development directions in the {EV} research ﬁeld, resulting in massive future and immediate {EV} penetration in the automotive market.},
	pages = {946},
	number = {5},
	journaltitle = {Energies},
	shortjournal = {Energies},
	author = {Varga, Bogdan and Sagoian, Arsen and Mariasiu, Florin},
	urldate = {2021-11-08},
	date = {2019-03-12},
	langid = {english},
	file = {Varga et al. - 2019 - Prediction of Electric Vehicle Range A Comprehens.pdf:/Users/philmoremorrison/Zotero/storage/XS26ZMW2/Varga et al. - 2019 - Prediction of Electric Vehicle Range A Comprehens.pdf:application/pdf},
}

@article{lin_monte_2017,
	title = {{MONTE} {CARLO} {TREE} {SEARCH} {AND} {MINIMAX} {COMBINATION} – {APPLICATION} {OF} {SOLVING} {PROBLEMS} {IN} {THE} {GAME} {OF} {GO}},
	url = {http://drum.lib.umd.edu/handle/1903/20449},
	doi = {10.13016/M2VD6P64Q},
	abstract = {Monte Carlo Tree Search ({MCTS}) has been successfully applied to a variety of games. Its best-first algorithm enables implementations without evaluation functions. Combined with Upper Confidence bounds applied to Trees ({UCT}), {MCTS} has an advantage over traditional depth-limited minimax search with alpha-beta pruning in games with high branching factors such as Go. However, minimax search with alpha-beta pruning still surpasses {MCTS} in domains like Chess. Studies show that {MCTS} does not detect shallow traps, where opponents can win within a few moves, as well as minimax search. Thus, minimax search performs better than {MCTS} in games like Chess, which can end instantly (king is captured). A combination of {MCTS} and minimax algorithm is proposed in this thesis to see the effectiveness of detecting shallow traps in Go problems.},
	author = {Lin, Jonathan Fun},
	urldate = {2021-11-22},
	date = {2017},
	langid = {english},
	note = {Publisher: Digital Repository at the University of Maryland},
	keywords = {Engineering, Monte Carlo Tree Search},
}


@misc{choudhary_introduction_2019,
	title = {Introduction to {Monte} {Carlo} {Tree} {Search}: {The} {Game}-{Changing} {Algorithm} behind {DeepMind}’s {AlphaGo}},
	url = {https://www.analyticsvidhya.com/blog/2019/01/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago/},
	abstract = {Monte Carlo Tree Search Tutorial the game changing algorithm behind deepmind alphago. In this article, learn about how Alphago and alphago program works.},
	language = {en},
	urldate = {2021-10-25},
	journal = {Analytics Vidhya},
	author = {Choudhary, Ankit},
	month = jan,
	year = {2019},
	file = {Snapshot:/Users/christopherfesta/Zotero/storage/7JJSJKPA/monte-carlo-tree-search-introduction-algorithm-deepmind-alphago.html:text/html},
}

@article{sutton_learning_1988,
	title = {Learning to {Predict} by the {Methods} of {Temporal} {Differences}},
	volume = {3},
	copyright = {COPYRIGHT 1988 Springer},
	issn = {0885-6125},
	abstract = {Byline: Richard S. Sutton (1) Keywords: Incremental learning; prediction; connectionism; credit assignment; evaluation functions This article introduces a class of incremental learning procedures specialized for prediction – that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage. Author Affiliation: Article History: Registration Date: 13/01/2005},
	language = {eng},
	number = {1},
	journal = {Machine learning},
	author = {Sutton, Richard S},
	year = {1988},
	note = {Publisher: Springer},
	pages = {9--},
}

@misc{karunakaran_actor-critic_2020,
	title = {The actor-{Critic} {Reinforcement} {Learning} algorithm},
	url = {https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14},
	abstract = {Policy-based and value-based RL algorithm},
	language = {en},
	urldate = {2021-11-16},
	journal = {Intro to Artificial Intelligence},
	author = {Karunakaran, Dhanoop},
	month = sep,
	year = {2020},
	file = {Snapshot:/Users/christopherfesta/Zotero/storage/W7JALQJS/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14.html:text/html},
}

@misc{goodrich_how_2021,
	title = {How {IBM}’s {Deep} {Blue} {Beat} {World} {Champion} {Chess} {Player} {Garry} {Kasparov}},
	url = {https://spectrum.ieee.org/how-ibms-deep-blue-beat-world-champion-chess-player-garry-kasparov},
	abstract = {The supercomputer could explore up to 200 million possible chess positions per second with its AI program},
	language = {en},
	urldate = {2021-12-10},
	journal = {IEEE Spectrum},
	author = {Goodrich, Joanna},
	month = jan,
	year = {2021},
	note = {Section: The Institute},
	file = {Snapshot:/Users/christopherfesta/Zotero/storage/GJTQK4YL/how-ibms-deep-blue-beat-world-champion-chess-player-garry-kasparov.html:text/html},
}

@online{abrams_lessons_2018,
	title = {Lessons From {AlphaZero} (part 4): Improving the Training Target},
	url = {https://medium.com/oracledevs/lessons-from-alphazero-part-4-improving-the-training-target-6efba2e71628},
	shorttitle = {Lessons From {AlphaZero} (part 4)},
	abstract = {This is the Fourth installment in our series on lessons learned from implementing {AlphaZero}. Check out Part 1, Part 2, and Part 3.},
	titleaddon = {Oracle Developers},
	author = {Abrams, Vish (Ishaya)},
	urldate = {2021-12-09},
	date = {2018-07-09},
	langid = {english},
}


@article{ring_replicating_nodate,
	title = {Replicating {DeepMind} {StarCraft} {II} Reinforcement Learning Benchmark with Actor-Critic Methods},
	abstract = {Reinforcement Learning ({RL}) is a subﬁeld of Artiﬁcial Intelligence ({AI}) that deals with agents navigating in an environment with the goal of maximizing total reward. Games are good environments to test {RL} algorithms as they have simple rules and clear reward signals. Theoretical part of this thesis explores some of the popular classical and modern {RL} approaches, which include the use of Artiﬁcial Neural Network ({ANN}) as a function approximator inside {AI} agent. In practical part of the thesis we implement Advantage Actor-Critic {RL} algorithm and replicate {ANN} based agent described in [Vinyals et al., 2017]. We reproduce the state-of-the-art results in a modern video game {StarCraft} {II}, a game that is considered the next milestone in {AI} after the fall of chess and Go.},
	pages = {40},
	author = {Ring, Roman},
	langid = {english},
	file = {Ring - Replicating DeepMind StarCraft II Reinforcement Le.pdf:/Users/philmoremorrison/Zotero/storage/ZB7ZMIAG/Ring - Replicating DeepMind StarCraft II Reinforcement Le.pdf:application/pdf},
}

@inproceedings{andrychowicz_what_2020,
	title = {What matters for on-policy deep actor-critic methods? a large-scale study},
	booktitle = {International Conference on Learning Representations},
	author = {Andrychowicz, Marcin and Raichuk, Anton and Stańczyk, Piotr and Orsini, Manu and Girgin, Sertan and Marinier, Raphaël and Hussenot, Leonard and Geist, Matthieu and Pietquin, Olivier and Michalski, Marcin and {others}},
	date = {2020},
	file = {Andrychowicz et al. - 2021 - WHAT MATTERS FOR ON-POLICY DEEP ACTOR- CRITIC METH.pdf:/Users/philmoremorrison/Zotero/storage/GDY88I5M/Andrychowicz et al. - 2021 - WHAT MATTERS FOR ON-POLICY DEEP ACTOR- CRITIC METH.pdf:application/pdf},
}

@article{chen_ucb_2017,
	title = {{UCB} Exploration via Q-Ensembles},
	url = {http://arxiv.org/abs/1706.01502},
	abstract = {We show how an ensemble of \$Q{\textasciicircum}*\$-functions can be leveraged for more effective exploration in deep reinforcement learning. We build on well established algorithms from the bandit setting, and adapt them to the \$Q\$-learning setting. We propose an exploration strategy based on upper-confidence bounds ({UCB}). Our experiments show significant gains on the Atari benchmark.},
	journaltitle = {{arXiv}:1706.01502 [cs, stat]},
	author = {Chen, Richard Y. and Sidor, Szymon and Abbeel, Pieter and Schulman, John},
	urldate = {2021-12-06},
	date = {2017-11-07},
	eprinttype = {arxiv},
	eprint = {1706.01502},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/philmoremorrison/Zotero/storage/ZWVZGW7C/Chen et al. - 2017 - UCB Exploration via Q-Ensembles.pdf:application/pdf;arXiv.org Snapshot:/Users/philmoremorrison/Zotero/storage/H8H2F7WY/1706.html:text/html},
}


@article{srinivasan_actor-critic_2018,
	title = {Actor-Critic Policy Optimization in Partially Observable Multiagent Environments},
	url = {https://arxiv.org/abs/1810.09026v5},
	abstract = {Optimization of parameterized policies for reinforcement learning ({RL}) is an important and challenging problem in artificial intelligence. Among the most common approaches are algorithms based on gradient ascent of a score function representing discounted return. In this paper, we examine the role of these policy gradient and actor-critic algorithms in partially-observable multiagent environments. We show several candidate policy update rules and relate them to a foundation of regret minimization and multiagent learning techniques for the one-shot and tabular cases, leading to previously unknown convergence guarantees. We apply our method to model-free multiagent reinforcement learning in adversarial sequential decision problems (zero-sum imperfect information games), using {RL}-style function approximation. We evaluate on commonly used benchmark Poker domains, showing performance against fixed policies and empirical convergence to approximate Nash equilibria in self-play with rates similar to or better than a baseline model-free algorithm for zero sum games, without any domain-specific state space reductions.},
	author = {Srinivasan, Sriram and Lanctot, Marc and Zambaldi, Vinicius and Perolat, Julien and Tuyls, Karl and Munos, Remi and Bowling, Michael},
	urldate = {2021-12-11},
	date = {2018-10-21},
	langid = {english},
	file = {Srinivasan et al. - 2018 - Actor-Critic Policy Optimization in Partially Obse.pdf:/Users/philmoremorrison/Zotero/storage/63C5S3XI/Srinivasan et al. - 2018 - Actor-Critic Policy Optimization in Partially Obse.pdf:application/pdf},
}


@online{karunakaran_actor-critic_2020,
	title = {The actor-Critic Reinforcement Learning algorithm},
	url = {https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14},
	abstract = {Policy-based and value-based {RL} algorithm},
	titleaddon = {Intro to Artificial Intelligence},
	author = {Karunakaran, Dhanoop},
	urldate = {2021-11-21},
	date = {2020-09-30},
	langid = {english},
	file = {Snapshot:/Users/philmoremorrison/Zotero/storage/X6TRJLC5/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14.html:text/html},
}

@online{pumperla_chapter_2019,
	title = {Chapter 12. Reinforcement learning with actor-critic methods · Deep Learning and the Game of Go},
	url = {https://livebook.manning.com/book/deep-learning-and-the-game-of-go/chapter-12/},
	abstract = {Using advantage to make reinforcement learning more efficient; Making a self-improving game {AI} with the actor-critic method; Designing and training multi-output neural networks in Keras;},
	titleaddon = {Manning Publications},
	author = {Pumperla, Max and Ferguson, Kevin},
	urldate = {2021-11-21},
	date = {2019},
	langid = {american},
	file = {Snapshot:/Users/philmoremorrison/Zotero/storage/EFAUNTZW/2.html:text/html},
}

@online{yoon_understanding_2019,
	title = {Understanding Actor Critic Methods},
	url = {https://towardsdatascience.com/understanding-actor-critic-methods-931b97b6df3f},
	abstract = {Preliminaries},
	titleaddon = {Medium},
	author = {Yoon, Chris},
	urldate = {2021-11-21},
	date = {2019-07-17},
	langid = {english},
	file = {Snapshot:/Users/philmoremorrison/Zotero/storage/DC89EQA7/understanding-actor-critic-methods-931b97b6df3f.html:text/html},
}

@inproceedings{konda_actor-critic_2000,
	title = {Actor-critic algorithms},
	pages = {1008--1014},
	booktitle = {Advances in neural information processing systems},
	author = {Konda, Vijay R and Tsitsiklis, John N},
	date = {2000},
	file = {Konda and Tsitsiklis - Actor-Critic Algorithms.pdf:/Users/philmoremorrison/Zotero/storage/KN9MTH6V/Konda and Tsitsiklis - Actor-Critic Algorithms.pdf:application/pdf},
}